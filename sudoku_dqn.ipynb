{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpLms6BQO_uQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SudokuEnvironment:\n",
        "    def __init__(self, initial_board):\n",
        "        self.board = initial_board\n",
        "        self.size = len(initial_board)\n",
        "\n",
        "    def reset(self):\n",
        "        return np.copy(self.board)\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col, number = action\n",
        "\n",
        "        if self.is_valid_move(action):\n",
        "            self.board[row][col] = number\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        done = self.is_done()\n",
        "\n",
        "        return np.copy(self.board), reward, done, {}\n",
        "\n",
        "    def is_done(self):\n",
        "        return np.all(self.board != 0)\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        return 1\n",
        "\n",
        "    def is_valid_move(self, action):\n",
        "        row, col, number = action\n",
        "\n",
        "        if (\n",
        "            number in self.board[row, :]\n",
        "            or number in self.board[:, col]\n",
        "            or number in self.get_subgrid(row, col)\n",
        "        ):\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_subgrid(self, row, col):\n",
        "        subgrid_row = (row // 2) * 2\n",
        "        subgrid_col = (col // 2) * 2\n",
        "        return self.board[subgrid_row:subgrid_row + 2, subgrid_col:subgrid_col + 2]\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "    def share_state_information(self, state, action, reward):\n",
        "        # Implement the logic for sharing state information between agents\n",
        "        # For simplicity, let's print a message indicating the information sharing\n",
        "        print(f\"Agent sharing information: State={state}, Action={action}, Reward={reward}\")\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.randint(self.state_size), np.random.randint(self.state_size), np.random.randint(1, 5)\n",
        "        else:\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for number in range(1, 10):\n",
        "                action_index = (state[0], state[1], number)\n",
        "                if self.q_table[action_index] > best_value:\n",
        "                    best_value = self.q_table[action_index]\n",
        "                    best_action = action_index\n",
        "\n",
        "            return best_action\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, alpha, gamma):\n",
        "        if np.all(next_state < self.state_size):\n",
        "            best_next_action = np.argmax(self.q_table[next_state, :])\n",
        "            self.q_table[state, action] += alpha * (reward + gamma * self.q_table[next_state, best_next_action] - self.q_table[state, action])\n",
        "        else:\n",
        "            print(f\"Invalid next_state value: {next_state}\")\n",
        "\n",
        "class MultiAgentSudokuSolver:\n",
        "    def __init__(self, board_size=4):\n",
        "        self.board_size = board_size\n",
        "        self.row_agent = QLearningAgent(board_size, board_size)\n",
        "        self.column_agent = QLearningAgent(board_size, board_size)\n",
        "        self.subgrid_agent = QLearningAgent(board_size, board_size)\n",
        "        self.validator_agent = QLearningAgent(board_size, board_size)\n",
        "        self.coordinator_agent = QLearningAgent(board_size, board_size)\n",
        "\n",
        "    def solve(self, sudoku_environment, num_episodes=1000):\n",
        "        epsilon = 1.0\n",
        "        alpha = 0.1\n",
        "        gamma = 0.9\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = sudoku_environment.reset()\n",
        "\n",
        "            while not sudoku_environment.is_done():\n",
        "                row_action = self.row_agent.choose_action(state, epsilon)\n",
        "                next_state, row_reward, _, _ = sudoku_environment.step(row_action)\n",
        "\n",
        "                column_action = self.column_agent.choose_action(state, epsilon)\n",
        "                _, column_reward, _, _ = sudoku_environment.step(column_action)\n",
        "\n",
        "                subgrid_action = self.subgrid_agent.choose_action(state, epsilon)\n",
        "                _, subgrid_reward, _, _ = sudoku_environment.step(subgrid_action)\n",
        "\n",
        "                validator_action = self.validator_agent.choose_action(state, epsilon)\n",
        "                _, validator_reward, _, _ = sudoku_environment.step(validator_action)\n",
        "\n",
        "                coordinator_action = self.coordinator_agent.choose_action(state, epsilon)\n",
        "                _, coordinator_reward, _, _ = sudoku_environment.step(coordinator_action)\n",
        "\n",
        "                self.row_agent.update_q_table(state, row_action, row_reward, next_state, alpha, gamma)\n",
        "                self.column_agent.update_q_table(state, column_action, column_reward, next_state, alpha, gamma)\n",
        "                self.subgrid_agent.update_q_table(state, subgrid_action, subgrid_reward, next_state, alpha, gamma)\n",
        "                self.validator_agent.update_q_table(state, validator_action, validator_reward, next_state, alpha, gamma)\n",
        "                self.coordinator_agent.update_q_table(state, coordinator_action, coordinator_reward, next_state, alpha, gamma)\n",
        "\n",
        "                self.row_agent.share_state_information(state, row_action, row_reward)\n",
        "                self.column_agent.share_state_information(state, column_action, column_reward)\n",
        "                self.subgrid_agent.share_state_information(state, subgrid_action, subgrid_reward)\n",
        "                self.validator_agent.share_state_information(state, validator_action, validator_reward)\n",
        "                self.coordinator_agent.share_state_information(state, coordinator_action, coordinator_reward)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            epsilon *= 0.995\n",
        "\n",
        "# Create a Sudoku environment\n",
        "initial_board = np.array([\n",
        "    [1, 0, 0, 0],\n",
        "    [3, 2, 4, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 0],\n",
        "\n",
        "])\n",
        "\n",
        "sudoku_environment = SudokuEnvironment(initial_board)\n",
        "\n",
        "multi_agent_solver = MultiAgentSudokuSolver()\n",
        "multi_agent_solver.solve(sudoku_environment)"
      ]
    }
  ]
}